{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABSTRACT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have parsed the resume to get the candidates name and the skills based on the skill list created. We have found the unigram and the bigrams and checked it with the skillset list. The output has been saved in the csv format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets see a sample resume that we have parsed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/ss/sample-resume-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/ss/sample-resume-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the sample resume which we have parsed to find the candidate name and the skills the candidate has. we have used four resume of different candidates having varied skillset to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the important libraries\n",
    "import docx, os\n",
    "import pandas as pd\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sample list of skills which we will use to find in each resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of skills\n",
    "skill=[]\n",
    "skill.append('python')\n",
    "skill.append('r')\n",
    "skill.append('data mining')\n",
    "skill.append('data analysis')\n",
    "skill.append('data modeling')\n",
    "skill.append('data visualization')\n",
    "skill.append('hadoop')\n",
    "skill.append('hive')\n",
    "skill.append('tableau')\n",
    "skill.append('power BI')\n",
    "skill.append('sas')\n",
    "skill.append('matlab')\n",
    "skill.append('scala')\n",
    "skill.append('pig')\n",
    "skill.append('mapreduce')\n",
    "skill.append('spark')\n",
    "skill.append('hadoop')\n",
    "skill.append('machine learning')\n",
    "skill.append('statistics')\n",
    "skill.append('excel')\n",
    "skill.append('aws')\n",
    "skill.append('azure')\n",
    "skill.append('mongodb')\n",
    "skill.append('sql')\n",
    "skill.append('pl/sql')\n",
    "skill.append('nosql')\n",
    "skill.append('mysql')\n",
    "skill.append('mssql')\n",
    "skill.append('oracle')\n",
    "skill.append('hbase')\n",
    "skill.append('ms acess')\n",
    "skill.append('dashboards')\n",
    "skill.append('agile')\n",
    "skill.append('innovator')\n",
    "skill.append('critical thinking')\n",
    "skill.append('docker')\n",
    "skill.append('leader')\n",
    "skill.append('master degree')\n",
    "skill.append('java')\n",
    "skill.append('big data')\n",
    "skill.append('cassandra')\n",
    "skill.append('qlik sense')\n",
    "skill.append('qlik view')\n",
    "skill.append('phd')\n",
    "skill.append('bachelor degree')\n",
    "skill.append('numpy')\n",
    "skill.append('tensorflow')\n",
    "skill.append('keras')\n",
    "skill.append('pandas')\n",
    "skill.append('sci-kit learn')\n",
    "skill.append('matplotlib')\n",
    "skill.append('seaborn')\n",
    "skill.append('deep learning')\n",
    "skill.append('classification')\n",
    "skill.append('decision tree')\n",
    "skill.append('clustering')\n",
    "skill.append('regression')\n",
    "skill.append('forcasting')\n",
    "skill.append('kpi')\n",
    "skill.append('linux')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe to store the name and skills from each resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe to store the details of resume\n",
    "final=pd.DataFrame(columns=['Name','Skills'],index=(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning the default vale to name column \n",
    "final['Name']= 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning the default value to skill column\n",
    "final['Skills']='skillset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method will be used to process the text by removing the punctuation marks and replace it by the blank space to prepare the text for finding the ngrams.\n",
    "\n",
    "It first converts all the characters in the text to lowercases. After that, it replaces commas, forward slashes, brackets and full stops with single whitespaces. Finally, it uses the split function on the text to split words by spaces and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a method to process the text \n",
    "def process_text(text):\n",
    "    \n",
    "        text = text.lower()\n",
    "        text = text.replace(',', ' ')\n",
    "        text = text.replace('/', ' ')\n",
    "        text = text.replace('(', ' ')\n",
    "        text = text.replace(')', ' ')\n",
    "        text = text.replace('.', ' ')\n",
    "        text = text.replace('[', ' ')\n",
    "        text = text.replace(']', ' ')\n",
    "        text = text.replace('\"', ' ')\n",
    "                        \n",
    " # Convert text string to a list of words\n",
    "        return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT IS N-GRAM?**\n",
    "\n",
    "n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.\n",
    "\n",
    "The ngram method will take the word list(processed text) and on the basis n(1,2,3) it will find the ngrams and save in the ngrams_list. It will first convert the list into a string and then from the first word it will start finding the gram.\n",
    "\n",
    "Upon receiving the input parameters, the generate_ngrams function declares a list to keep track of the generated n-grams. It then loops through all the words in words_list to construct n-grams and appends them to ngram_list.\n",
    "\n",
    "When the loop completes, the generate_ngrams function returns ngram_list back to the caller.\n",
    "\n",
    "-- pip install ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.techcoil.com/blog/how-to-generate-n-grams-in-python-without-using-any-external-libraries/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/ss/N-gram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a method to count the ngrams of the text\n",
    "def generate_ngrams(words_list, n):\n",
    "    ngrams_list = []\n",
    " \n",
    "    for num in range(0, len(words_list)):\n",
    "        ngram = ' '.join(words_list[num:num + n])\n",
    "        ngrams_list.append(ngram)\n",
    " \n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next automate the script to read all the word files stored in a document.\n",
    "We read each word file and then for each file, loop through all the paragarphs to find the cadidate name and the skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://automatetheboringstuff.com/chapter13/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the directory to the location where the sample resumes have been saved\n",
    "os.chdir('cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AYUSH JAIN</td>\n",
       "      <td>[mongodb, sql, oracle, java, qlik sense, qlik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POOJITH SHANKAR SHETTY</td>\n",
       "      <td>[sql, oracle, java, aws, nosql, mongodb, machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prashant Reddy</td>\n",
       "      <td>[python, oracle, machine learning, big data, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHWETA PATHAK</td>\n",
       "      <td>[big data, data mining, sql, oracle, pl/sql, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N</td>\n",
       "      <td>skillset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>N</td>\n",
       "      <td>skillset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>N</td>\n",
       "      <td>skillset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>N</td>\n",
       "      <td>skillset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>N</td>\n",
       "      <td>skillset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>N</td>\n",
       "      <td>skillset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name                                             Skills\n",
       "0              AYUSH JAIN  [mongodb, sql, oracle, java, qlik sense, qlik ...\n",
       "1  POOJITH SHANKAR SHETTY  [sql, oracle, java, aws, nosql, mongodb, machi...\n",
       "2          Prashant Reddy  [python, oracle, machine learning, big data, a...\n",
       "3           SHWETA PATHAK  [big data, data mining, sql, oracle, pl/sql, j...\n",
       "4                       N                                           skillset\n",
       "5                       N                                           skillset\n",
       "6                       N                                           skillset\n",
       "7                       N                                           skillset\n",
       "8                       N                                           skillset\n",
       "9                       N                                           skillset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parsing the resume to find for all the words present in the skill list defined above\n",
    "fullText=[]\n",
    "#find all the files in the specified location\n",
    "for filename in os.listdir('.'):\n",
    "    #find all the files with '.docx'\n",
    "    if filename.endswith('.docx'):\n",
    "        doc = docx.Document(filename)\n",
    "        i=0\n",
    "        skillset=[]\n",
    "        flat_list_one=[]\n",
    "        two=[]      \n",
    "        # for each paragraph in the document\n",
    "        for para in doc.paragraphs:\n",
    "            fullText.append(para.text) \n",
    "            \n",
    "            #running the loop till it goes through all the paragraphs of the document\n",
    "            while(i<len(doc.paragraphs)):\n",
    "                a = doc.paragraphs[i].text\n",
    "                a= a.lower()\n",
    "                words = a.split()\n",
    "                #finding the unigram words\n",
    "                for item in skill:\n",
    "                    for word in words:\n",
    "                        if item == word:\n",
    "                            if item not in skillset:\n",
    "                                skillset.append(word)\n",
    "                \n",
    "                words_list = process_text(a)\n",
    "                # finding the bigrams in the document\n",
    "                bigrams = generate_ngrams(words_list, 2)\n",
    "                two.append(bigrams)\n",
    "                for sublist in two:\n",
    "                    for item in sublist:\n",
    "                        flat_list_one.append(item)\n",
    "                for words in flat_list_one:\n",
    "                    for items in skill:\n",
    "                        if items == words:\n",
    "                            if items not in skillset:\n",
    "                                skillset.append(words)\n",
    "                i=i+1\n",
    "    \n",
    "    k=0\n",
    "    #assuming the first text in the document would be a resume, so finding the first not blank paragraph\n",
    "    name= doc.paragraphs[k].text\n",
    "    while(name is \"\" ):\n",
    "            k=k+1\n",
    "            name=doc.paragraphs[k].text         \n",
    "\n",
    "\n",
    "    j = 0\n",
    "    #assigning the name and the skills\n",
    "    while(j<len(final)):\n",
    "        if final['Name'][j] != name:\n",
    "            if final['Name'][j]=='N':\n",
    "                y = j\n",
    "                final['Name'][y]= name\n",
    "                final['Skills'][y]=skillset\n",
    "                name=' '\n",
    "                j=j+1\n",
    "                break\n",
    "            else:\n",
    "                j=j+1\n",
    "        else:\n",
    "            j=j+1\n",
    "            break\n",
    "        \n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the duplicate value in the dataframe\n",
    "final=final.drop_duplicates(subset='Name',keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AYUSH JAIN</td>\n",
       "      <td>[mongodb, sql, oracle, java, qlik sense, qlik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POOJITH SHANKAR SHETTY</td>\n",
       "      <td>[sql, oracle, java, aws, nosql, mongodb, machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prashant Reddy</td>\n",
       "      <td>[python, oracle, machine learning, big data, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHWETA PATHAK</td>\n",
       "      <td>[big data, data mining, sql, oracle, pl/sql, j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name                                             Skills\n",
       "0              AYUSH JAIN  [mongodb, sql, oracle, java, qlik sense, qlik ...\n",
       "1  POOJITH SHANKAR SHETTY  [sql, oracle, java, aws, nosql, mongodb, machi...\n",
       "2          Prashant Reddy  [python, oracle, machine learning, big data, a...\n",
       "3           SHWETA PATHAK  [big data, data mining, sql, oracle, pl/sql, j..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the dataframe\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change back to the original directory\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe as the csv file\n",
    "final.to_csv('resume_parser.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we haved parsed through four sample resumes and stored the results of candidates name and skillset in the csv file. We used unigram and bigram to match the skill in the sample skills list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTRIBUTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have written 90% of the code on our own. Where ever we have used the code from any location we have cited the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LICENSE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is Licensed under the MIT license and is documented by Ayush Jain and Shweta Pathak.\n",
    "\n",
    "https://opensource.org/licenses/MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Ayush Jain and Shweta Pathak\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
